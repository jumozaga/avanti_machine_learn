{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Configuração Inicial"
   ],
   "metadata": {
    "collapsed": false,
    "id": "gTwmW32LdbDo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Configuração e import de bibliotecas."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5tpyf1WZdbDp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import getcwd\n",
    "from tensorflow import keras\n",
    "from funcoes_locais import reajustar_imagem, processar_imagem\n",
    "\n",
    "diretorio_treino = 'asl-alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
    "diretorio_teste = 'asl-alphabet/asl_alphabet_test/'\n",
    "diretorio_atual = getcwd()"
   ],
   "metadata": {
    "id": "Ct74BCICdbDp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importação dos dados - via diretório\n",
    "\n",
    "Essa é a forma ideal de importar os dados, já que ela permite a separação das bases de treino e validação no momento da importação.\n",
    "Porém, precisamos pegar apenas uma parte das bases de treino e validação para fins de velocidade do treino.\n",
    "Seguindo o downloado e redimensionamento feito na parte de pré-processamento, cada classe terá 150 amostras."
   ],
   "metadata": {
    "id": "CeWSVpgaKqj4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora, será montado o dataset de treino, validação e teste, usando keras."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = keras.utils.image_dataset_from_directory(\n",
    "    diretorio_treino,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(200, 200),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.25,\n",
    "    subset='training',\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = keras.utils.image_dataset_from_directory(\n",
    "    diretorio_treino,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(200, 200),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.25,\n",
    "    subset='validation',\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    diretorio_teste,\n",
    "    labels=list(range(0,28)),\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(200, 200),\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArWkm5P29Mm3",
    "outputId": "d7316100-07dc-4674-e0f3-202ddea23278"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tendo as imagens carregadas, vamos aplicar a função de pré-processamento, para assim podermos alimentar o modelo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = train_dataset.map(lambda x, y: tf.py_function(processar_imagem, [x, y], [tf.float32, tf.int32]))\n",
    "val_dataset = val_dataset.map(lambda x, y: (tf.py_function(processar_imagem, [x, y], [tf.float32, tf.uint32])))\n",
    "test_dataset = test_dataset.map(lambda x, y: (tf.py_function(processar_imagem, [x, y], [tf.float32, tf.int32])))"
   ],
   "metadata": {
    "id": "aci-85NOHutT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda x, y: reajustar_imagem(x, y))\n",
    "val_dataset = val_dataset.map(lambda x, y: reajustar_imagem(x, y))\n",
    "test_dataset = test_dataset.map(lambda x, y: reajustar_imagem(x, y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#  Configuração de Pipeline\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "id": "8z6uS0mHT_Mn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "num_classes = 29\n",
    "\n",
    "pretrained_base = tf.keras.applications.inception_v3.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(200, 200, 3),\n",
    "    pooling='max',\n",
    "    classes=num_classes,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "\n",
    "pretrained_base.Trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    pretrained_base,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.00001,\n",
    "    monitor='val_loss',\n",
    "    patience=16,\n",
    "    restore_best_weights=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "check_point = ModelCheckpoint(\n",
    "    diretorio_atual,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.7,\n",
    "                              patience=5,\n",
    "                              min_lr=0.0001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.007),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data = val_dataset,\n",
    "  batch_size=32,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stopping, check_point, reduce_lr]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
